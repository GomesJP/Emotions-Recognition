{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "SIZE_FACE = 48\n",
    "CASC_PATH = './data/haarcascade_frontalface_default.xml'\n",
    "EMOTIONS = ['angry', 'disgusted', 'fearful', 'happy', 'sad', 'surprised', 'neutral']\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = load_model('./data/keras_model.h5')\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(CASC_PATH)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# FIFO list of lenght 10\n",
    "many_pred = deque(maxlen=10)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=8,\n",
    "        minSize=(35, 35),\n",
    "        flags=cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "\n",
    "    # Draw a rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        # resize and normalize for prediction using our CNN\n",
    "        gray_face = cv2.resize(gray[y:(y+h), x:(x+w)], (SIZE_FACE, SIZE_FACE), interpolation = cv2.INTER_CUBIC) / 255.\n",
    "        gray_face = gray_face.reshape([-1, SIZE_FACE, SIZE_FACE, 1])\n",
    "\n",
    "        prediction = model.predict(gray_face)\n",
    "        \n",
    "        # average predicitons over 10 frames. You do not need to do this, but it makes the model much more stable.\n",
    "        # However, it also means that only one person can be in the stream at a time.\n",
    "        many_pred.appendleft(prediction)\n",
    "        mean_prediction = np.mean(many_pred, axis=0)\n",
    "        \n",
    "        index=np.argmax(mean_prediction)\n",
    "\n",
    "        emotion_str = 'emotion: ' + EMOTIONS[index]\n",
    "        probability_str = 'probability: ' + str(np.around(mean_prediction[0][index]*100,2)) + '%'\n",
    "\n",
    "        # draw text emotion\n",
    "        cv2.putText(frame,emotion_str,(x, y-10), font, 0.5,(255,255,255),1,cv2.LINE_AA)\n",
    "        cv2.putText(frame,probability_str,(x, y-30), font, 0.5,(255,255,255),1,cv2.LINE_AA)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:emotions-recognition]",
   "language": "python",
   "name": "conda-env-emotions-recognition-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
